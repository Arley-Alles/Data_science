{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------- Dataset manipulation ---------------------------------#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#------------------------------------- Graphics -----------------------------------------#\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#--------------------------------- Features balancing -----------------------------------#\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "#---------------------------------- Machine learning ------------------------------------#\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset initial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this initial information we can see \"bmi\" feature has some missing values. In order to fill these values let's take a look in some other features and see if they are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = [\"age\", \"bmi\", \"avg_glucose_level\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize = (12, 7))\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    sns.distplot(x = df[continuous[i]], ax = ax, axlabel = continuous[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 7))\n",
    "\n",
    "correlation = df[continuous].corr()\n",
    "sns.heatmap(correlation, cmap='viridis', annot=True, vmax = 1.0,\n",
    "            vmin = 0.3, linewidths=.5, data = df[continuous])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix show the linear correlation among continuous variables are not so strong. But maybe there is a non-linear correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize = (17, 7))\n",
    "continuous = [\"age\", \"bmi\", \"avg_glucose_level\"]\n",
    "\n",
    "sns.scatterplot(x = df[\"age\"], y = df[\"bmi\"], ax = fig.axes[0], data = df, hue = \"stroke\")\n",
    "sns.scatterplot(x = df[\"age\"], y = df[\"avg_glucose_level\"], ax = fig.axes[1], data = df, hue = \"stroke\")\n",
    "sns.scatterplot(x = df[\"avg_glucose_level\"], y = df[\"bmi\"], ax = fig.axes[2], data = df, hue = \"stroke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between bmi and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = df.columns\n",
    "continuous = [\"id\", \"age\", \"avg_glucose_level\", \"bmi\", \"_\"]\n",
    "\n",
    "cont = 0\n",
    "for j, cat in enumerate(categorical):\n",
    "    \n",
    "        if(cat != continuous[cont]):\n",
    "            print(\"#---------------------\", cat, \"-----------------------#\")\n",
    "            print(df.groupby([cat])[\"bmi\"].mean())\n",
    "            print(\"\")\n",
    "            \n",
    "        else:\n",
    "            cont += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between age and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = df.columns\n",
    "continuous = [\"id\", \"age\", \"avg_glucose_level\", \"bmi\", \"_\"]\n",
    "\n",
    "cont = 0\n",
    "for j, cat in enumerate(categorical):\n",
    "    \n",
    "        if(cat != continuous[cont]):\n",
    "            print(\"#---------------------\", cat, \"-----------------------#\")\n",
    "            print(df.groupby([cat])[\"age\"].mean())\n",
    "            print(\"\")\n",
    "            \n",
    "        else:\n",
    "            cont += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting point in these results is a possible relationship between \"work_type\" and \"bmi\". People classified as 'children' has an average age of 6.8 years, and a low 'bmi'. This is totally accepted because children have a low body mass index. For the case 'Never_worked' the age is about 16 years, revealing a considered amount of teenagers. This group should have a low 'bmi', although it has to be higher than 'children' group. The remained three other groups of \"work_type\" seems to have a good proportion of adults, so these groups must take almost the same 'bmi' and higher than 'children' and 'Never_worked'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values on \"bmi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_children = df[\"work_type\"] == \"children\"\n",
    "filter_teenager = df[\"work_type\"] == \"Never_worked\"\n",
    "\n",
    "df[\"bmi\"][filter_children] = df[\"bmi\"][filter_children].fillna(20.03)\n",
    "df[\"bmi\"][filter_teenager] = df[\"bmi\"][filter_teenager].fillna(25.54)\n",
    "df[\"bmi\"] = df[\"bmi\"].fillna(30.30)\n",
    "print(\"Total de valores missing em 'bmi': \", df[\"bmi\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking \"Unknown's\" amount in 'gender' and 'smoking_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#------------------------- Gender ------------------------------#\")\n",
    "print(df[\"gender\"].value_counts())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"#------------------------- Smoking ------------------------------#\")\n",
    "print(df[\"smoking_status\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 7))\n",
    "sns.countplot(data = df, x = \"smoking_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'gender' situation is easier than 'smoking_status' because there is only one case classificated as 'Other'. In 'smoking_status' there are 1544 cases classified as 'Unknown' which corresponds to 30.21% of the entire feature. \n",
    "\n",
    "For \"smoking_status\" a possible solution might be replace the \"Unknown\" values for \"never smoked\" status. At the statistical point of view, \"never smoked\" status is the most seen variable on the dataset. The physical explanation for this choice is people have been knowing the dangers about smoking, so many people avoid it. But in a first moment let's keep this classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between stroke cases and continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_num = [\"age\", \"bmi\", \"avg_glucose_level\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize = (20, 12))\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    g = sns.barplot(data = df, y = features_num[i], x = \"stroke\", ci = \"sd\",\n",
    "                    capsize=.2, ax = ax)\n",
    "    \n",
    "    g.set_title(features_num[i], fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize = (20, 12))\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    g = sns.boxplot(data = df, y = features_num[i], x = \"stroke\",\n",
    "                    ax = ax)\n",
    "    \n",
    "    g.set_title(features_num[i], fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until this point we can see that variable \"age\" is very correlated with stoke classification. So we must take an especial attention about this variable. We might filter these variable using some other features. For exemple, is a little bit difficult to find a child less than 15 years who smoking or formerly smoked. Another point is a 'bmi' above 35 for a 15 years old is also hard to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------- Filtering \"age\" feature ----------------------------------# \n",
    "filter_age      = df[\"age\"] <= 15\n",
    "filter_smoking  = df[\"smoking_status\"] == \"smokes\"  \n",
    "filter_smoking2 = df[\"smoking_status\"] == \"formerly smoked\"\n",
    "filter_bmi      = df[\"bmi\"] >= 35\n",
    "filter_gender   = df[\"gender\"] == \"Other\"\n",
    "\n",
    "array_filter = df[(filter_age & filter_smoking) | (filter_age & filter_smoking2) | (filter_age & filter_bmi) | (filter_gender)].index\n",
    "\n",
    "new_df = df.drop(array_filter)\n",
    "new_df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between stroke cases and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var = [\"gender\", \"hypertension\", \"heart_disease\", \"ever_married\", \"work_type\", \n",
    "           \"Residence_type\",\"smoking_status\"]\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize = (17, 12))\n",
    "cont = 0\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    \n",
    "    if cont < (7):\n",
    "        sns.countplot(x = cat_var[i], hue = \"stroke\", data = new_df, ax = ax)\n",
    "        \n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countplot is a way to see possible correlations between variables, however I think it isn't an easy one. Another form to check correlation is the frequency of stroke and no stroke cases in each feature's classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- Encoding hypertension and heart_disease features ------------------------#\n",
    "new_df[\"New_hypertension\"] = new_df[\"hypertension\"].map({0: \"no_hyper\", 1: \"hyper\"})\n",
    "new_df[\"New_heart_disease\"] = new_df[\"heart_disease\"].map({0: \"no_disease\", 1: \"disease\"})\n",
    "new_df[\"New_ever_married\"] = new_df[\"ever_married\"].map({\"Yes\": \"married\", \"No\": \"no_married\"})\n",
    "\n",
    "#------------------------------------- List of categorical features ----------------------------------------#\n",
    "cat_var = [\"gender\", \"New_hypertension\", \"New_heart_disease\", \"New_ever_married\", \"work_type\", \n",
    "           \"Residence_type\",\"smoking_status\"]\n",
    "\n",
    "#--------------------- Extract proportion of stoke cases in each feature's classification ------------------#\n",
    "dict_prop = {}\n",
    "for i, var in enumerate(cat_var):\n",
    "    total = 0\n",
    "    \n",
    "    for j, classif in enumerate(new_df[var].unique()):\n",
    "        \n",
    "        for k in range(2): #for stroke classification\n",
    "            if j < len(new_df[var][new_df[\"stroke\"] == k].value_counts()):\n",
    "                total += new_df[var][new_df[\"stroke\"] == k].value_counts()[j]\n",
    "        \n",
    "        prop_no_stroke =  new_df[var][df[\"stroke\"] == 0].value_counts()[j]/total\n",
    "        \n",
    "        if j < len(new_df[var][new_df[\"stroke\"] == k].value_counts()):\n",
    "            prop_stroke =  new_df[var][df[\"stroke\"] == 1].value_counts()[j]/total\n",
    "            dict_prop[classif] = [prop_no_stroke, prop_stroke]\n",
    "            \n",
    "        else:\n",
    "            dict_prop[classif] = [prop_no_stroke, 0.0]\n",
    "            \n",
    "        total = 0\n",
    "\n",
    "#------------------------- Converting dictionary of proportions to dataframe ------------------------------#\n",
    "df_prop = pd.DataFrame(dict_prop)\n",
    "df_prop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------- Parameters of Pie plot --------------------------------------------#\n",
    "n = 10\n",
    "m = 2\n",
    "fig , ax = plt.subplots(n, m, figsize = (15, 25))\n",
    "cont = 0\n",
    "var = df_prop.columns\n",
    "explode = (0, 0.1)  \n",
    "\n",
    "\n",
    "#------------------------------------------ Iterative plot -----------------------------------------------#\n",
    "for i in range(n):\n",
    "    for j in range(m):\n",
    "        \n",
    "        if cont < ((n*m)-1):\n",
    "            ax[i,j].pie(df_prop[var[cont]], labels = [\"No_stroke\", \"Stroke\"], explode=explode,\n",
    "                        autopct='%1.1f%%', shadow=True, startangle=60)\n",
    "        \n",
    "            ax[i,j].axis('equal')\n",
    "            ax[i,j].set_title(df_prop.columns[cont], loc = \"left\", fontsize = 16)\n",
    "        \n",
    "        cont +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is easy to see what features are correlated with stroke cases. For example the features \"hypertension\" and \"heart_disease\" are correlated with stroke. The proportion of \"stroke\" and \"no stroke\" cases are different when we change the classification of these features. On the other hand, the feature \"gender\" doens't seem make much difference about stroke cases, once the frequency in both classifications are almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of stroke and not stroke cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------- Parameters of Pie plot ---------------------------------------------#\n",
    "plt.figure(figsize = (12, 7))\n",
    "labels = [\"No\", \"Yes\"]\n",
    "explode = (0, 0.13)\n",
    "count_stroke = new_df[\"stroke\"].value_counts()\n",
    "text = {\"color\": 'w', \"fontsize\": 16}\n",
    "\n",
    "\n",
    "#---------------------------------------------- Pie plot -----------------------------------------------------#\n",
    "plt.pie(count_stroke, labels = labels, \n",
    "        explode = explode, autopct='%1.1f%%', \n",
    "        shadow=True, startangle = 0,textprops = text)\n",
    "\n",
    "\n",
    "#------------------------------------------- Pie plot legend -------------------------------------------------#\n",
    "plt.legend(labels,\n",
    "          title=\"Classification\",\n",
    "          loc=\"upper right\",\n",
    "           prop={'size': 18},\n",
    "           bbox_to_anchor=(0.7, 0., 0.75, 0.7))\n",
    "\n",
    "plt.title(\"Frequency of stroke cases\", fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very clear there is an imbalance for stroke target. This might be a problem for model construction. In this situation we have to balance this target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------- Features to be encodered -------------------------------#\n",
    "columns = [\"Residence_type\", \"gender\", \"hypertension\", \"heart_disease\", \n",
    "           \"ever_married\",\"work_type\", \"smoking_status\"]\n",
    "\n",
    "\n",
    "#-------------------------------- Features to be eliminated -------------------------------#\n",
    "eliminate = [\"stroke\", \"id\", \"New_hypertension\", \n",
    "             \"New_heart_disease\", \"New_ever_married\"]\n",
    "\n",
    "\n",
    "#-----------------------------------Target and Dataframe ----------------------------------#\n",
    "target = new_df[\"stroke\"]\n",
    "new_df = new_df.drop(columns = eliminate)\n",
    "\n",
    "\n",
    "#---------------------------------------- Encoder -----------------------------------------#\n",
    "le = preprocessing.OrdinalEncoder()\n",
    "new_df[columns] = le.fit_transform(new_df[columns])\n",
    "\n",
    "\n",
    "#------------------------------------Train and test data ----------------------------------#\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_df, target, \n",
    "                                                    test_size = 0.4, random_state = 0)\n",
    "\n",
    "\n",
    "#------------------------------------ Target balancing ------------------------------------#\n",
    "oversample = SMOTE()\n",
    "x, y = oversample.fit_resample(x_train, y_train)\n",
    "\n",
    "plt.figure(figsize = (12, 7))\n",
    "sns.countplot(x = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    \n",
    "    RFC = RandomForestClassifier(random_state = 0)\n",
    "    DTC = DecisionTreeClassifier(random_state = 0)\n",
    "    GNB = GaussianNB()\n",
    "    SVM = SVC(random_state = 0)\n",
    "    GBC = GradientBoostingClassifier(random_state = 0)\n",
    "    RC  = RidgeClassifier(random_state = 0)\n",
    "    \n",
    "    #---------------------------------- Fit models --------------------------------#\n",
    "    model_RFC = RFC.fit(x_train, y_train)\n",
    "    model_DTC = DTC.fit(x_train, y_train)\n",
    "    model_GNB = GNB.fit(x_train, y_train)\n",
    "    model_SVM = SVM.fit(x_train, y_train)\n",
    "    model_GBC = GBC.fit(x_train, y_train)\n",
    "    model_RC  = RC.fit(x_train, y_train)\n",
    "    \n",
    "    #--------------------------------- Predictions --------------------------------#\n",
    "    prd_RFC = model_RFC.predict(x_test)\n",
    "    prd_DTC = model_DTC.predict(x_test)\n",
    "    prd_GNB = model_GNB.predict(x_test)\n",
    "    prd_SVM = model_SVM.predict(x_test)\n",
    "    prd_GBC = model_GBC.predict(x_test)\n",
    "    prd_RC  = model_RC.predict(x_test)\n",
    "    \n",
    "    #-------------------------------- Accuracy ------------------------------------#\n",
    "    acc_RFC = accuracy_score(y_test, prd_RFC)\n",
    "    acc_DTC = accuracy_score(y_test, prd_DTC)\n",
    "    acc_GNB = accuracy_score(y_test, prd_GNB)\n",
    "    acc_SVM = accuracy_score(y_test, prd_SVM)\n",
    "    acc_GBC = accuracy_score(y_test, prd_GBC)\n",
    "    acc_RC  = accuracy_score(y_test, prd_RC)\n",
    "    accuracy = [acc_RFC, acc_DTC, acc_GNB, acc_SVM, acc_GBC, acc_RC]\n",
    "    \n",
    "    #---------------------------- Confusion matrix --------------------------------#\n",
    "    matrix_RFC = confusion_matrix(y_test, prd_RFC)\n",
    "    matrix_DTC = confusion_matrix(y_test, prd_DTC)\n",
    "    matrix_GNB = confusion_matrix(y_test, prd_GNB)\n",
    "    matrix_SVM = confusion_matrix(y_test, prd_SVM)\n",
    "    matrix_GBC = confusion_matrix(y_test, prd_GBC)\n",
    "    matrix_RC  = confusion_matrix(y_test, prd_RC)\n",
    "    matrix = [matrix_RFC, matrix_DTC, matrix_GNB, matrix_SVM, matrix_GBC, matrix_RC]\n",
    "    \n",
    "    return accuracy, matrix\n",
    "\n",
    "\n",
    "#----------------------------- Applying 'model' function --------------------------#\n",
    "accuracy, matrix = model(x, x_test, y, y_test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of each preditive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(acc, labels):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (12, 7))\n",
    "    x = np.arange(len(labels))\n",
    "\n",
    "    ax.barh(x, accuracy)\n",
    "    ax.set_yticks(x)\n",
    "    ax.set_yticklabels(labels, fontsize = 16)\n",
    "    ax.set_xlabel(\"Accuracy\", fontsize = 16)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#------------------------------ Applying 'plot' function ----------------------------#    \n",
    "labels = ('Random Forest', 'Decision Tree', 'Naive Bayes', \n",
    "          'Support Vector Machine', \"Gradient Boosting\",\n",
    "          \"Ridge Regression\") \n",
    "\n",
    "plot(accuracy, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix for each preditive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize = (12, 7))\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    \n",
    "    if i < len(matrix):\n",
    "        g = sns.heatmap(matrix[i], annot=True ,fmt='', ax = ax)\n",
    "        \n",
    "        g.set_title(labels[i])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an interesting point. Accuracy tells us 'Random Forest' and 'Gradient Boosting' as best models. However, taking a look at confusion matrix we can see these models are good for classificate no_stroke cases. On the ohter hand they are very bad for classificate a stroke case. In a physical point of view is much better a model with good detection in stroke case (because is terrible disease) then a no_stroke case. So, \"Naive Bayes\", \"Ridge\" and \"SVM\" are more suitable models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
